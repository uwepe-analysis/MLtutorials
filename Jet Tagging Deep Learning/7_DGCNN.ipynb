{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data Preprocessing\n",
    "## 1. Loading data\n",
    "There are several common datatypes: .h5 , .pkl , .json , .txt and compacted datatypes .z , .gz , .zip ,etc. <br>\n",
    "Generally, we tend to use .h5, because it takes up relatively small space.\n",
    "\n",
    "Note we are using the .z five labels file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "path = \" \"\n",
    "f = h5py.File(path, 'r')\n",
    "f.keys()                    # check keys in .h5 file, we need to read it by the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# extract the data array\n",
    "darray = f['whatever_key_you_find'][()] \n",
    "\n",
    "# we would like to use pandas to manipulate the data.\n",
    "features = ['f1','f2','f3']         \n",
    "labels = ['l1','l2']\n",
    "data_feature = pd.DataFrame(darray, columns=features)\n",
    "data_label = pd.DataFrame(darray, columns=labels)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you are familiar with your data, there is a short cut.\n",
    "path = \" \"\n",
    "with h5py.File(path, 'r') as f:\n",
    "    darray = f['whatever_key_you_find'][()]\n",
    "    data = pd.DataFrame(darray, columns= columns_you_want)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of DGCNN, we are gonna use 7 features and 5 labels( labels depend on what task you are doing) as our input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Features(7) | Labels(5) |\n",
    "|:---|:--- |\n",
    "|\"j1_etarel\" -- delta eta, |'J_t',|\n",
    "|\"j1_phirel\" -- delta phi, |'J_q'|\n",
    "|\"log(j1_pt)\" -- log pt, |'J_g'|\n",
    "|\"log(j1_e)\" -- log E, |'J_w'|\n",
    "|\"log(j1_ptrel)\" -- log(pt / ptjet), |'J_z'|\n",
    "|\"log(j1_erel)\" -- log(E / Ejet), ||\n",
    "|\"j1_deltaR\" -- delta R||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j1_etarel: ration of the eta of each constituent to the eta of the jet<br>\n",
    "j1_phirel: ratio of the phi of each constituent to the phi of the jet<br>\n",
    "j1_pt: constituent pt (transverse momentum)<br>\n",
    "j1_e: constituent energy<br>\n",
    "j1_ptrel: ratio of the pT of each constituent to the pT of the jet<br>\n",
    "j1_erel: ration of the energy of each constituent of the energy of the jet<br>\n",
    "j1_deltaR: sqrt((Δeta)2 + (Δ phi)2 ) <br><br>\n",
    "j_g: gluon jet<br>\n",
    "j_q: quark jet <br>\n",
    "j_w: W boson jet <br>\n",
    "j_z: Z boson jet<br>\n",
    "j_t: Top jet<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise: \n",
    "Read out all the columns and try to understand what they are.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature construction\n",
    "We cannot get the log values directly from the original file, therefore a little feature construction is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feature[\"log(j1_pt)\"] = np.log(data_feature['j1_pt'])\n",
    "data_feature[\"log(j1_e)\"] = np.log(data_feature[\"j1_e\"])\n",
    "data_feature[\"log(j1_ptrel)\"] = np.log(data_feature['j1_ptrel'])\n",
    "data_feature[\"log(j1_erel)\"] = np.log(data_feature['j1_erel'])\n",
    "\n",
    "data_feature.drop(['j1_pt','j1_e','j1_ptrel','j1_erel'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine the features and labels so that we can send it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.concat([data_feature,data_label],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Downsizing jets\n",
    "In the data we got, the number of constituents contained in each jet is different, ranging from 20 to 200. While we need a fixed size as input in the machine learning process, that is to say, we need to manually specify the number of constituents for each jet. If we set nConstituents = 40, all Jets whose number of constituents is less than 40 will be zero-padded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) How do we identify jets\n",
    "In the data I have contacted, there are two forms: particle-based and jet-based. <br>\n",
    "\n",
    "For the particle-based data, there should be a feature help identify the data. For example \"j_index\", it tells you the unique index of a jet. Get it <a href=\"https://drive.google.com/file/d/1DCpxWbWtqU4sQwmGbZTg-4cdGAWonDKy/view?usp=sharing\">here</a>.<br>\n",
    "\n",
    "For the jet-based data, each row represents a jet, you can get specific number of constinuents by conditional slicing. Get it <a href=\"https://zenodo.org/record/2603256#.X62WkFqSmbh\">here</a>.<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) N-Constituents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels+['j_index']\n",
    "data_label = pd.DataFrame(darray, columns=labels)\n",
    "data_all = pd.concat([data_feature, data_label],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def data_transform (nConstituents, data_all):\n",
    "    kColumns = data_all.columns.shape[0]\n",
    "\n",
    "    # we expect the output shape (mJets, nConstituents, kColumns)\n",
    "    jet_list = list(set(data_all['j_index']))\n",
    "    data_expected = []\n",
    "\n",
    "    for jet in tqdm(jet_list):\n",
    "        # Zero padding for insufficient jets. \n",
    "        # So we create a empty array and add signals in.\n",
    "        jet_frame = np.zeros((nConstituents, kColumns))\n",
    "        jet_temp = data_all[data_all['j_index']==jet].values\n",
    "        if (jet_temp.shape[0]<nConstituents):\n",
    "            for i, constituent in enumerate(jet_temp):\n",
    "                jet_frame[i] = constituent\n",
    "        else:\n",
    "            jet_frame += jet_temp[:nConstituents]\n",
    "        data_expected.append(jet_frame)\n",
    "\n",
    "    # \"j_index\" is useless for machine learning part. Drop it!\n",
    "    return np.array(data_expected[:,:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_transform(40, data_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not the only solution or the fastest function to accomplish the goal. You can try to develop a better one. If you find a better method, please share to your collegues. Because we are gonna use this method for almost all the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Excercise\n",
    "Try to think how you can get the same data shape with a jet-based data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Test Split\n",
    "We rely on the sklearn package to accomplish it. There is a build-in function.<br>\n",
    "Choose a random seed and use it for all your researches. Wanna know why? To keep Consistent input very time you run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "Apply the code above to your data. For further explanations for parameters, Google it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check the array shape in this cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Creating a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'lib')\n",
    "import classes\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model from classes.py file, and change the parameters according to our reshaped data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = classes.EdgeConvClassifier((40, 7)).model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compile the model, and set the learning rate. You can all change other settings like optimizer and loss function. Print the model structre to check each layers have the right parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "            optimizer=keras.optimizers.Adam(lr = 0.0001), \n",
    "            loss='categorical_crossentropy', \n",
    "            metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkpO3j1GwrRg"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3LHoqKEwrRg"
   },
   "source": [
    "Train the model using model.fit() function, and set the validation_split value, number of epochs,and bastch_size. Batch_size represents the size of data bins used to train the network, since with large volumes of data it cannot fit all onto your RAM at one time. An epoch is one iteration through the entire shuffled data set; with additional epochs, the data is reshuffled and used to train the network again. The validation split represents the fraction of the remaining training data to use as a validation set during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sv216d2VwrRg"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "        batch_size=1024,\n",
    "        validation_split=0.25,\n",
    "        epochs=10, \n",
    "        shuffle = True, \n",
    "        callbacks = None,\n",
    "        use_multiprocessing=True, \n",
    "        workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDhDc936wrRg"
   },
   "source": [
    "After training the model, you can save the result to you local directory using model.save() funciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mW9NEqUgwrRg"
   },
   "outputs": [],
   "source": [
    "model.save('DGCNN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9fRczFdwrRg"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjwriXjUwrRg"
   },
   "source": [
    "Now to validate the result, plot the learning curve: loss on the training set versus the loss on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dg7H2qUswrRg"
   },
   "outputs": [],
   "source": [
    "def learningCurveLoss(history):\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'], linewidth=1)\n",
    "    plt.plot(history.history['val_loss'], linewidth=1)\n",
    "    plt.title('Model Loss over Epochs')\n",
    "    plt.legend(['training sample loss','validation sample loss'])\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C05B7Y3pwrRg"
   },
   "outputs": [],
   "source": [
    "learningCurve(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKqgo_H5wrRg"
   },
   "source": [
    "Then plot the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BpanUz-wrRg"
   },
   "outputs": [],
   "source": [
    "if 'j_index' in labels:\n",
    "    labels = labels[:-1]\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "def makeRoc(features_val, labels_val, labels, model, outputSuffix=''):\n",
    "    labels_pred = model.predict(features_val)\n",
    "    df = pd.DataFrame()\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    auc1 = {}\n",
    "    plt.figure()       \n",
    "    for i, label in enumerate(labels):\n",
    "        df[label] = labels_val[:,i]\n",
    "        df[label + '_pred'] = labels_pred[:,i]\n",
    "        fpr[label], tpr[label], threshold = roc_curve(df[label],df[label+'_pred'])\n",
    "        auc1[label] = auc(fpr[label], tpr[label])\n",
    "        plt.plot(fpr[label],tpr[label],label='%s tagger, AUC = %.1f%%'%(label.replace('j_',''),auc1[label]*100.))\n",
    "    plt.xlabel(\"Background Efficiency\")\n",
    "    plt.ylabel(\"Signal Efficiency\")\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim(0.001,1.05)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('%s ROC Curve'%(outputSuffix))\n",
    "    #plt.savefig('%s_ROC_Curve.png'%(outputSuffix))\n",
    "    return labels_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Tz8j117wrRg"
   },
   "outputs": [],
   "source": [
    "y_pred = makeRoc(X_test, y_test, labels, model, outputSuffix='DGCNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_IwGGvtwrRg"
   },
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxsRWYS7wrRg"
   },
   "source": [
    "After getting a reasonable learning curve and ROC curve, you can start to change and modify the parameters like learning rate and number of epochs, or even the hyperparameters inside the model. Find the best result from this DGCNN model with our data. And try to identify any anomalies and explain why it happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
